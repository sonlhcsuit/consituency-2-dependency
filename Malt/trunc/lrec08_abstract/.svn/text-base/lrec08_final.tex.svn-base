	\documentclass[10pt, a4paper]{article}
\usepackage{lrec2006}
\usepackage{graphicx}


%\usepackage{times}
%\usepackage{latexsym}
%\usepackage{array}
%\usepackage{natbib}

\title{MaltEval: An Evaluation and Visualization Tool for Dependency Parsing}
\name{Jens Nilsson$^*$, Joakim Nivre$^{*\dag}$} 

\address{$^*$V{\"a}xj{\"o} University, School of Mathematics and Systems Engineering, Sweden\\
$^{\dag}$Uppsala University, Dept. of Linguistics and Philology, Sweden\\
  jens.nilsson@vxu.se, joakim.nivre@vxu.se\\
}
               
               
\abstract{This paper presents a freely available evaluation tool for dependency parsing, MaltEval (http://w3.msi.vxu.se/users/jni/malteval). It is flexible and extensible, and provides functionality for both quantitative evaluation and visualization of dependency structure. The quantitative evaluation is compatible with other standard evaluation software for dependency structure which does not produce visualization of dependency structure, and can output more details as well as new types of evaluation metrics. In addition, MaltEval has generic support for confusion matrices. It can also produce statistical significance tests when more than one parsed file is specified. The visualization module also has the ability to highlight discrepancies between the gold-standard files and the parsed files, and it comes with an easy to use GUI functionality to search in the dependency structure of the input files.}

\begin{document}
\maketitleabstract

\section{Introduction}

Dependency parsing in natural language processing has become popular in recent years, both within the NLP research community and as a component in various NLP systems. As a consequence, the CoNLL shared tasks of 2006~\cite{CoNLL2006} and 2007~\cite{CoNLL2007} focused on multilingual dependency parsing using dependency-based treebanks, something that has led to a further increase 
in the popularity of dependency-based parsing methods.

An important component in the two CoNLL shared tasks and in many NLP applications is evaluation, both quantitative and qualitative. There are very few widely used quantitative evaluation software tools for dependency parsing. One exception is eval.pl,\footnote{And eval07.pl, the evaluation script of the shared task 2007 containing minor modifications compared to eval.pl} the evaluation script in Perl created by the organizers of the first CoNLL shared task. It was used for evaluating the output of the participants' parsers, mainly using the quantitative metric labeled attachment score (LAS). Using the same evaluation software is a way of guaranteeing that different systems are compared fairly.

The script eval.pl also provides functionality for a more detailed error analysis in a quantitative fashion, such as computing accuracy for individual part-of-speech tags, and precision and recall for individual dependency types, arc depth and arc direction, etc. However, in order to get a deeper understanding of the errors that a parser makes, investigating the errors of individual sentences is also important, as this can be part of the process of improving the parser. This type of qualitative error analysis can be facilitated by visualizing the dependency structure. The script eval.pl does not have this functionality, making it unsuitable for this type of qualitative error analysis.

However, visualizing the dependency structure is a functionality that several NLP software tools provide. Some examples are Tree Editor (TrEd)~\cite{HajicHladkaPajas2001}, which is a graphical editor and viewer of trees and annotation tool written in Perl. Another visualization tool is NetGraph~\cite{Jiri2007}, a tool for searching in treebanks in the FS format, the format used for encoding e.g. the Prague Dependency Treebank. None of these have support for the data format used in the shared tasks, the CoNLL format, which has become a de facto standard format for parsing dependency structure.

A visualization tool that supports the CoNLL format is DepSVG~\cite{DepSVG}. However, it only has the ability to produce vector-based image files, one file for each dependency graph. This makes it tedious to use for browsing between the dependency graphs of different sentences.

Another drawback of these visualization tools is that they do not provide functionality for quantitative evaluation like eval.pl. Moreover, none of the tools can visually compare the parse trees of the gold-standard and the output of a parser by highlighting mismatches, a useful functionality for manual qualitative visual error analysis.

\section{MaltEval 1.0}

MaltEval is a new software tool written in Java that combines quantitative and qualitative evaluation in one tool. It is to a large extent adapted to the evaluation script eval.pl, as the functionality of MaltEval is essentially a superset of this script (when used in the most verbose mode).\footnote{Currently, the statistics under the heading ``Local contexts involved in several frequent errors'' in the output of eval.pl cannot be produced, and some other information is presented differently.} It is also more flexible and contains many additional features.

Like the above mentioned visualization tools, MaltEval is able to visualize dependency structure. Unlike them, it provides visual support for qualitative evaluation by highlighting errors.

\subsection{Quantitative Evaluation}
Here is a list of features that are implemented in MaltEval, but are lacking in eval.pl:\footnote{Everything is described in more detail in the User Guide that is shipped with the MaltEval distribution.}
%\begin{itemize}
\paragraph{Flexibility} MaltEval comes with default evaluation settings, which can be manipulated through flags or files containing the evaluation settings. For instance, MaltEval is executed with default settings like this:

\begin{scriptsize}
\begin{verbatim}
  java -jar MaltEval.jar -s parser.conll -g gold.conll
\end{verbatim}
\end{scriptsize}

The same evaluation can also be achieved by

\begin{scriptsize}
\begin{verbatim}
  java -jar MaltEval.jar --GroupBy Token --Metric LAS
                      -s parser.conll -g gold.conll
\end{verbatim}
\end{scriptsize}

where the grouping strategy (see below) and the type of metric instead are explicitly specified using flags. Another equivalent way of producing the same evaluation is like this:

\begin{scriptsize}
\begin{verbatim}
  java -jar MaltEval.jar -e eval.xml
                      -s parser.conll -g gold.conll
\end{verbatim}
\end{scriptsize}

if the file eval.xml contains:

\begin{scriptsize}\begin{verbatim}
  <evaluation>
    <parameter name="Metric"><value>LAS</value>
    </parameter>
    <parameter name="GroupBy"><value>Token</value>
    </parameter>
  </evaluation>
\end{verbatim}\end{scriptsize}

Any one of the more than 25 parameters can be specified using either flags or in an evaluation file. The result is by default written to standard output, or to a file by specifying an output file.
\paragraph{Several parsed files} MaltEval can not only evaluate single parsed dependency files, but also automatically evaluate multiple files as well as perform automatic evaluation of cross-validation experiments. The script eval.pl is only limited to evaluating a single parsed file at a time.
\paragraph{File formats} Supports a number of dependency-based file formats, such as the CoNLL and Malt-XML formats.
\paragraph{GroupBy} MaltEval has a large number of grouping strategies for tokens. Besides the default attachment score evaluation (correct tokens / number of tokens), there are currently 23 grouping strategies, such as ArcLength, ArcDepth, BranchingFactor, ArcProjectivity and Frame. Essentially, all the detailed results that eval.pl can produce can also be produced by using the appropriate grouping strategy, specified using the \texttt{--GroupBy} flag.

For instance, grouping by dependency label could produce something like this

\begin{scriptsize}\begin{verbatim}
  Metric-> LAS
  GroupBy-> Deprel

  ===================================

  precision     recall     Deprel
  ----------------------------------
  0.618         0.488      Row mean
  21            24         Row count
  ----------------------------------
  -             0          AdvAtr
  0.5           0.091      Apos
  0.809         0.802      Atr
  0.5           0.333      AtrAdv
  -             0          AtrAtr
  0.556         0.484      Atv
  -             0          AtvV
  ...
\end{verbatim}\end{scriptsize}
where the precision and recall are displayed for all dependency labels in the entire parsed files. The rows labeled \texttt{Row mean} shows the mean of all dependency labels and \texttt{Row count} the number of district dependency labels that appeared in the parsed data (under precision) and in the gold-standard file (under recall). MaltEval is also able to compute other types of attributes such as F-score.

Here is another example, grouping by ArcProjectivity:
\begin{scriptsize}
\begin{verbatim}
  java -jar MaltEval.jar --Groupby ArcProjectivity
                      -s parser.conll -g gold.conll
\end{verbatim}
\end{scriptsize}
which can result in the following output:
\begin{scriptsize}\begin{verbatim}
  Metric-> LAS
  GroupBy-> ArcProjectivity

  ========================================

  precision     recall     ArcProjectivity
  ----------------------------------------
  0.853         0.868      Proj
  0.795         0.145      Non-proj
\end{verbatim}\end{scriptsize}
where the precision and recall of projective and non-projective arcs are reported. 

\paragraph{Metric} Specify whether to evaluate labeled or unlabeled attachment score or label accuracy, as well as other metrics such as error rate for the values of the head and dependency label. It is for instance possible to measure the error rate of tokens located close to an erroneous token by typing
\begin{scriptsize}
\begin{verbatim}
  java -jar MaltEval.jar --Metric AnyWrong
      --GroupBy Clustering -s parser.conll -g gold.conll
\end{verbatim}
\end{scriptsize}
which could yield the output:
\begin{scriptsize}\begin{verbatim}
  Metric-> AnyWrong
  GroupBy-> Clustering

  =====================================

  accuracy /    Relative token position
  -------------------------------------
  0.223         -5
  0.225         -4
  0.24          -3
  0.24          -2
  0.288         -1
  -             0
  0.28          1
  0.244         2
  0.244         3
  0.232         4
  0.233         5
\end{verbatim}\end{scriptsize}
The metric AnyWrong means that either the head or the dependency label of a token is incorrect. Each row is the average error rate of all tokens at a certain position in relation to an incorrect token. All types of metrics and grouping strategies, are described in more details in the User Guide (see the conclusion section). 

\paragraph{Exclude Sentence by Length} Support for excluding sentences longer and/or shorter than specified lengths.
\paragraph{Exclude Tokens by Attribute} Support for excluding various tokens from the evaluation based on e.g. word form or part-of-speech.
\paragraph{Formatting Flexibility} Flexibility to format the evaluation output, such as disabling or enabling detailed output.
\paragraph{Confusion Matrix} Possibility to automatically produce confusion matrices for any grouping strategy. Here is an example of such a confusion matrix:

\begin{scriptsize}\begin{verbatim}
  Confusion matrix for ArcDirection
  left    right   to_root   Col: system / Row: gold
  -------------------------------------------------
  -       1581    64        left
  1033    -       60        right
  359     245     -         to_root\end{verbatim}\end{scriptsize}

Another confusion matrix will be displayed if one simply changes the grouping strategy to e.g. Deprel, showing how the parser makes mistakes in assigning the dependency types.
\paragraph{Statistical Significance} Possibility to automatically test statistical significance if one gold-standard file and two or more parsed file are specified. A piece of the statistically significant result could look like this

\begin{scriptsize}\begin{verbatim}
  <1>   <2>   <3>   McNemar: p<0.01?
  --------------------------------------
  -     1     0     <1> (parser1.conll)
  -     -     1     <2> (parser2.conll)
  -     -     -     <3> (parser3.conll)
\end{verbatim}\end{scriptsize}

where McNemar's test has been used pairwise between three parsed files for LAS (1=there is a statistically significant difference for $p<0.01$).

\paragraph{Batching} Support for batched evaluation, i.e. possibility to perform several types of evaluations in sequence as in eval.pl, but with a greater flexibility of including and excluding different types of evaluation settings.

\subsection{Qualitative Evaluation}

\begin{figure}[t]
\begin{center}
\includegraphics*[width=0.4\textwidth]{chinese}
\caption{Chinese sentence visualized using MaltEval}
\label{fig:deptree}
\end{center}
\end{figure}
The integrated visualization module is enabled by simply switching on the visualization flag, which creates a window with possibility to browse though the dependency graphs. The visualization of the dependency structure in MaltEval is illustrated in figure~\ref{fig:deptree}. It contains all the information that is present in the CoNLL format, either in the picture directly or as pop-up labels when the mouse pointer is located over the tokens. 

The example also illustrates that MaltEval has full support for Unicode, as the sentence in the particular example is a Chinese sentence displayed using a Chinese font. MaltEval will investigate the tokens of the input file, and as long as there is at least one appropriate font installed on the computer, MaltEval will choose one of these fonts.

In case the dependency graph contains non-projectivity, such as in the top dependency graph of figure~\ref{fig:highlightErrors}, it will contain crossing arcs (which might make it easier to spot them than in the dependency structure of TrEd, NetGraph and SVGDep, which do not display crossing arcs).

Figure~\ref{fig:highlightErrors} also illustrates how mismatches are highlighted in the parsed file(s) in order to easier perform a manual error analysis. Mismatches are highlighted using both colors (red labels and arcs) and dashed lines for arcs and dependency labels individually.

\begin{figure*}[t]
\begin{center}
\includegraphics*[width=0.9\textwidth]{maltTreebankViewer}
\caption{Visualizing many parsed dependency files, and highlighting errors.}
\label{fig:highlightErrors}
\end{center}
\end{figure*}

\subsection{Searching in Visualization Mode}

The toolbar in figure~\ref{fig:highlightErrors} shows another useful functionality. The visualization module can be used for searching the gold-standard and parsed files. A common way to perform a search is:
\begin{enumerate}
\item Choose in which file to search in. This is done by selecting the gold-standard or on one of the parsed files in the leftmost combo box in the toolbar.
\item Choose what type of information to search for in the second combo box. This combo box contains a list of all GroupBy-alternatives.
\item When (1) and (2) have been selected, the third combo box becomes populated with all values for the chosen GroupBy-alternative in the chosen input file. The user then selects a value from the list.
\item The search is started by clicking on the search button, and a list of all sentences having a least one hit is presented to the user in a fourth combo box to the right of the reach button. There is also a negation check box, which instead lists all other sentences having no hit if it is checked.
\end{enumerate}

For instance, the gold-standard and the GroupBy-alternative Deprel with value \emph{ADV} have been selected in figure~\ref{fig:highlightErrors}. In the result combo box, the displayed sentence has been selected, where the token with the dependency label \emph{ADV} is highlighted using a thicker arc and token font.

The search tool offers a lot of flexibility, since the second combo box is directly tied to all GroupBy strategies. Here are some examples:
\begin{itemize}
\item With ArcProjectivity all projective or non-projective sentences can be selected.
\item With BranchingFactor, all sentences with a token having a certain number of children can be displayed.
\item With ArcDepth, all sentences having especially deeply nested arcs can be selected.
\item It is of course also possible to search for specific words or parts-of-speech.
\end{itemize}

It is worth noting that the combo box for the values (the third combo box) has full support for regular expressions. This entails among other things that complex searches can be performed. One simple example is to search for all tokens that have a word form ending with an \emph{a} (by typing .*a), which e.g. will highlight tokens 3 and 6 in the figure.

\subsection{Plug-ins, API and Javadoc}

Yet another flexible property of MaltEval is the possibly for users with knowledge in Java to write their own grouping strategies. This can be very handy if a user needs one type of grouping strategy that is currently not implemented in MaltEval. The User Guide, which contains information about how to implement, compile and archive a plug-in, comes with the distribution of MaltEval. A comprehensive Javadoc for the MaltEval API is also provided, something that is necessary for developers of plug-ins. A developer must essentially only implement a Java interface that all grouping strategies must do, which is also explained in details in the User Guide.

\subsection{A Comparison to eval.pl}

Even though eval.pl has some shortcomings in term of qualitative evaluation, it is like a ``standard'' tool for evaluating dependency structure. It is therefore important that a new evaluation tool of dependency structure is compatible to eval.pl. We have already mentioned that the functionality of MaltEval essentially is a superset of eval.pl. It is crucial that their common output is the same, let aside differences in the formatting and sorting of the output.

Such an evaluation has been performed, showing that MaltEval is compatible with eval.pl. The comparison was performed for a of number of parsers and languages from the CoNLL-X shared task. Due to the large amount of output produced by both evaluators, a complete listing of the output cannot be presented here. However, to simplify comparison, a special eval.pl-flag has been implemented in MaltEval, which produces the same type of information as eval.pl (except the content under the ``Local contexts involved in several frequent errors''). All evaluation files that this special flag uses are enumerated in appendix~\ref{app:eval}.

\section{Conclusion}
\label{sec:conclusion}
We present an evaluation tool for dependency parsers that combines quantitative evaluation and visualization, which is a property that no tools that we are aware of have. It is flexible in the sense that it comes with a large number of parameters that are easy to modify. It is also flexible and extensible in the sense that new grouping strategies can be plugged into MaltEval even without access to the source code of MaltEval. It is freely available for download and use, but it comes with no guarantees.\footnote{http://w3.msi.vxu.se/users/jni/malteval/}


\bibliographystyle{lrec2006}
\bibliography{reading}

\appendix
\section{Evaluation Files Replicating eval.pl}
\label{app:eval}
Below are the ten evaluation files that can be applied in order to reproduce the output of eval.pl including all punctuation (i.e. the output of eval07.pl). Each item of the list corresponds to one header in the output of eval.pl. The evaluation files are shipped with the distribution. For simplicity, a special eval07.pl value for the \texttt{-e} flag has been implemented, which runs all evaluation files of the distribution in sequence:
\begin{scriptsize}
\begin{verbatim}
  java -jar MaltEval.jar -e eval07.pl
                      -s parser.conll -g gold.conll
\end{verbatim}
\end{scriptsize}
\begin{itemize}

\item Overall accuracy:
\begin{scriptsize}
\begin{verbatim}
<evaluation>
  <parameter name="Metric">
    <value>LAS</value>
    <value>UAS</value>
    <value>LA</value>
  </parameter>
  <parameter name="GroupBy">
    <value>Token</value>
  </parameter>
  <formatting argument="pattern" format="0.00\%"/>
</evaluation>
\end{verbatim}
\end{scriptsize}

\item Accuracy and its distribution over CPOSTAGs:
\begin{scriptsize}
\begin{verbatim}
<evaluation>
  <parameter name="Metric">
    <value>LAS</value>
    <value>UAS</value>
    <value>LA</value>
  </parameter>
  <parameter name="GroupBy">
    <value format="all|counter-">Cpostag</value>
  </parameter>
  <formatting argument="pattern" format="0\%"/>
</evaluation>
\end{verbatim}
\end{scriptsize}

\item Error rate and its distribution over CPOSTAGs:
\begin{scriptsize}
\begin{verbatim}
<evaluation>
  <parameter name="Metric">
    <value>HeadWrong</value>
    <value>LabelWrong</value>
    <value>BothWrong</value>
  </parameter>
  <parameter name="GroupBy">
    <value format="all|counter-">Cpostag</value>
  </parameter>
  <formatting argument="pattern" format="0\%"/>
</evaluation>
\end{verbatim}
\end{scriptsize}

\item Precision and recall of DEPREL:
\begin{scriptsize}
\begin{verbatim}
<evaluation>
  <parameter name="Metric">
    <value>LabelRight</value>
  </parameter>
  <parameter name="GroupBy">
  <value format="precision|recall|parsercounter|
treebankcounter|correctcounter">Deprel</value>
  </parameter>
  <formatting argument="pattern" format="0.00\%"/>
</evaluation>
\end{verbatim}
\end{scriptsize}

\item Precision and recall of DEPREL + ATTACHMENT:
\begin{scriptsize}
\begin{verbatim}
<evaluation>
  <parameter name="Metric">
    <value>LAS</value>
  </parameter>
  <parameter name="GroupBy">
    <value format="precision|recall|parsercounter|
treebankcounter|correctcounter">Deprel</value>
  </parameter>
  <formatting argument="pattern" format="0.00\%"/>
</evaluation>
\end{verbatim}
\end{scriptsize}

\item Precision and recall of binned HEAD direction:
\begin{scriptsize}
\begin{verbatim}
<evaluation>
  <parameter name="Metric">
    <value>DirectionRight</value>
  </parameter>
  <parameter name="GroupBy">
    <value format="precision|recall|parsercounter|
treebankcounter|correctcounter">ArcDirection</value>
  </parameter>
  <formatting argument="pattern" format="0.00\%"/>
</evaluation>
\end{verbatim}
\end{scriptsize}

\item Precision and recall of binned HEAD distance:
\begin{scriptsize}
\begin{verbatim}
<evaluation>
  <parameter name="Metric">
    <value>GroupedHeadToChildDistanceRight</value>
  </parameter>
  <parameter name="GroupBy">
    <value format="precision|recall|parsercounter|
treebankcounter|correctcounter">GroupedRelationLength
</value>
  </parameter>
  <formatting argument="pattern" format="0.00\%"/>
</evaluation>
\end{verbatim}
\end{scriptsize}

\item Frame confusions:
\begin{scriptsize}
\begin{verbatim}
<evaluation>
  <parameter name="Metric">
    <value>LA</value>
  </parameter>
  <parameter name="GroupBy">
    <value format="treebankcounter-5">Frame</value>
  </parameter>
  <formatting argument="pattern" format="0.00%"/>
  <formatting argument="confusion-matrix" format="1"/>
</evaluation>
\end{verbatim}
\end{scriptsize}

\item (1) 5 focus words where most of the errors occur, (2) one-token preceeding contexts where most of the errors occur, (3) two-token preceeding contexts where most of the errors occur, (4) one-token following contexts where most of the errors occur, (5) two-token following contexts where most of the errors occur:
\begin{scriptsize}
\begin{verbatim}
<evaluation>
  <parameter name="Metric">
    <value>AnyWrong</value>
    <value>LabelWrong</value>
    <value>HeadWrong</value>
    <value>BothWrong</value>
  </parameter>
  <parameter name="GroupBy">
    <value format="correctcounter-5">
Cpostag@0#Wordform@0</value>
    <value format="correctcounter-5">
Cpostag@-1</value>
    <value format="correctcounter-5">
Cpostag@-1#Wordform@-1</value>
    <value format="correctcounter-5">
Cpostag@-2#Cpostag@-1</value>
    <value format="correctcounter-5">
Cpostag@-2#Cpostag@-1#Wordform@-2#Wordform@-1</value>
    <value format="correctcounter-5">
Cpostag@1</value>
    <value format="correctcounter-5">
Cpostag@1#Wordform@1</value>
    <value format="correctcounter-5">
Cpostag@2#Cpostag@1</value>
    <value format="correctcounter-5">
Cpostag@2#Cpostag@1#Wordform@2#Wordform@1</value>
  </parameter>
  <formatting argument="pattern" format="0.00%"/>
</evaluation>
\end{verbatim}
\end{scriptsize}

\item (1) Sentence with the highest number of word errors, (2) Sentence with the highest number of head errors, (3) Sentence with the highest number of dependency errors:
\begin{scriptsize}
\begin{verbatim}<evaluation>
  <parameter name="Metric">
    <value>AnyWrong</value>
    <value>LabelWrong</value>
    <value>HeadWrong</value>
  </parameter>
  <parameter name="GroupBy">
    <value format="correctcounter-5">Sentence</value>
  </parameter>
  <formatting argument="pattern" format="0.00%"/>
  <formatting argument="details" format="1"/>
</evaluation>
\end{verbatim}
\end{scriptsize}

\end{itemize}

\end{document}
